<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Unknown </title></head><body>
<h1 id="week-01">Week 01</h1>
<ul>
<li>Introduction</li>
<li>Wtf?</li>
<li>Supervised/ Unsupervised</li>
<li>Linear regression with one variable</li>
<li>Model and Cost Function<ul>
<li>Model representation</li>
<li>Cost function</li>
</ul>
</li>
<li>Parameter Learning<ul>
<li>Gradient descent</li>
<li>Gradient descent for linear regression</li>
</ul>
</li>
<li>Linear algebra</li>
</ul>
<hr />
<h2 id="i-intro">I. Intro</h2>
<h3 id="1-defintions">1. Defintions:</h3>
<ul>
<li>Arthur Samuel, 59' =&gt; ML gives computer ability to learn without being explicitly programmed; </li>
<li>Tom Mitchell, 98': a computer is said to learn from experience (E) with respect to task (T) and some performances (P), if its performances on T as measured on by (P) improves (E).</li>
<li>Example: </li>
<li>Email program watches you mark or no as Spam, and learns how to better filter Spam.<ul>
<li>T = Classify emails</li>
<li>E = Watching you label emails as spam or not spam</li>
<li>P = The number of emails correctly classified</li>
</ul>
</li>
<li>
<p>Playing checkers.</p>
<ul>
<li>T = playing</li>
<li>E = play many games</li>
<li>P = Probablilty that program will win the next game</li>
</ul>
</li>
<li>
<p>ML algo.: Supervised and unsupervised learning + (bonus: reinforcement &amp; recommender) </p>
</li>
</ul>
<h3 id="2-supervised-ml">2. Supervised ML</h3>
<p>Right answers are given and task of algorithm is to produce more right answers.</p>
<p>There's a relationship between the input and the output.</p>
<ul>
<li>Example: </li>
<li>Beast cancer:</li>
<li>It is called classification problem: label sizes to 1 for malignant or 0 for benign.</li>
<li>pTo Predict results in a discrete output.</li>
<li>→ classification is about predicting a label.</li>
<li>Housing price prediction:</li>
<li>It is called regression problem to predict continuous valued output (prices).</li>
<li>Price as a function of size is a continuous output.</li>
<li>Map input variables to some continuous function.</li>
<li>→ regression is about predicting a quantity.</li>
<li>Can turn to into classification problem: output whether the house "sells for more or less than the asking price." </li>
<li>Here we are classifying the houses based on price into two discrete categories.</li>
<li>Age</li>
<li>Given a picture, predict the age.</li>
</ul>
<h3 id="3-unsupervised-ml">3. Unsupervised ML</h3>
<ul>
<li>Approach problems with little or no idea what our results should look like.</li>
<li>Can derive results structure by clustering it based on relationships among the variables in the input.</li>
<li>No feedback based on the prediction results.</li>
<li>Example</li>
<li>Cocktail party problem: identify human voices and music =&gt; Non-clustering.</li>
<li>Discover market segments and group customers into different market segments.</li>
<li>Group articles into sets about the same stories.</li>
<li>Create groups from a collection of 1,000,000 different genes based on lifespan, location, roles, etc.</li>
</ul>
<h2 id="ii-model-and-cost-function">II. Model and cost function</h2>
<h3 id="1-model-representation">1. Model representation:</h3>
<ul>
<li>$x^{(i)}$ = “input” variables = input features.</li>
<li>$y^{(i)}$ = “output” variables = target variable.
  =&gt; example: predict price $(y)$ from living area $(x)$</li>
<li>couple $(x^{(i)}, y^{(i)})$ is called training example.</li>
<li>couples $(x^{(i)}, y^{(i)}); i= 1..m$ is called training set.</li>
<li>
<p>$X$ space of input values; $Y$ space of output values; $X = Y = \mathbb{R}$</p>
</li>
<li>
<p>Supervised learning problem:</p>
</li>
<li>given a training set, to learn $h: X → Y$ so h(x) ≈ y</li>
<li>$h$ is called <strong>hypothesis</strong></li>
</ul>
<p>[[ An algorithm will learn from a training set how to predict $y$ when $x$ is provided using $h$ ]]</p>
<p>Such as in our housing example, we call the learning problem a regression problem.</p>
<ul>
<li>Goal is to found $h$ and its parameters as $h(x)= ax + b$</li>
</ul>
<h3 id="2-cost-function">2. Cost function</h3>
<ul>
<li>Cost function = average difference of all results of all $x$'s &amp; $y$'s</li>
<li>$ J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=0..m}{(h_\theta(x^{(i)}) - y^{(i)})^2};$</li>
<li>
<p>$m$ is the number of training examples</p>
</li>
<li>
<p>Hypothesis $h_\theta(x) = \theta_0 + \theta_1x$</p>
</li>
<li>Parameters: $\theta_0, \theta_1$</li>
<li>Choose these parameters so $h_\theta(x) $ is close to $y$ for the training couple $(x, y)$ </li>
<li>Goal: $minimize J(\theta_0, \theta_1)$</li>
</ul>
<p>Intuition 1:
  - Ideally, the line should pass through all the points of our training data set to minimize $J(\theta_0, \theta_1)$.
  - $h_\theta(x)$ is plotted : a linear function passing through some points of the training set.
  - Plot $h_\theta(x)$ and $ J(\theta_0, \theta_1)$ 
  - $\theta_1 = 0$       =&gt; $J(0) = 2.3$
- $\theta_1 = 0.5$    =&gt; $J(0.5) = 0.58$
  - $\theta_1 = 1$       =&gt; $J(1) = 0$
  - $\theta_1 = 1.5$    =&gt; $J(1.5) = 0.58$</p>
<ul>
<li>$\theta_1 = 2$       =&gt; $J(0) = 2.3$</li>
</ul>
<p>→ Thus as a goal, try to minimize the cost function. In this case, $\theta_1 =1$  is the global minimum.</p>
<h2 id="iii-parameter-learning">III. Parameter learning</h2>
<h3 id="1-gradient-descent">1. Gradient descent</h3>
<ul>
<li>Have some function $J(\theta_0, \theta_1)$</li>
<li>Want to $minimize J(\theta_0, \theta_1)$</li>
<li><strong>Outline</strong></li>
<li>Start with some $\theta_0, \theta_1$.</li>
<li>
<p>Keep changing $\theta_0, \theta_1$ to reduce $J(\theta_0, \theta_1)$ to find up a minimum.</p>
</li>
<li>
<p>Estimate the parameters in the hypothesis function → Gradient descent</p>
</li>
</ul>
</body></html>