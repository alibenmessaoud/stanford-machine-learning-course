Which of the following statements about diagnostics are true? Check all that apply.

It’s hard to tell what will work to improve a learning algorithm, so the best approach is to go with gut feeling and just see what works.

Diagnostics can give guidance as to what might be more fruitful things to try to improve a learning algorithm.

Diagnostics can be time-consuming to implement and try, but they can still be a very good use of your time.

A diagnostic can sometimes rule out certain courses of action (changes to your learning algorithm) as being unlikely to improve its performance significantly.

234

------

Suppose an implementation of linear regression (without regularization) is badly overfitting the training set. In this case, we would expect:

The training error $J(\theta)$ to be **low** and the test error $J_{\text{test}}(\theta)$ to be **high**

The training error $J(\theta)$ to be **low** and the test error $J_{\text{test}}(\theta)$ to be **low**

The training error $J(\theta)$ to be **high** and the test error $J_{\text{test}}(\theta)$ to be **low**

The training error $J(\theta)$ to be **high** and the test error $J_{\text{test}}(\theta)$ to be **high**

1

------

Consider the model selection procedure where we choose the degree of polynomial using a cross validation set. For the final model (with parameters \thetaθ), we might generally expect $J_{\text{CV}}(\theta)$ To be lower than $J_\text{test}(\theta) $because:

An extra parameter (d, the degree of the polynomial) has been fit to the cross validation set.

An extra parameter (d, the degree of the polynomial) has been fit to the test set.

The cross validation set is usually smaller than the test set.

The cross validation set is usually larger than the test set.

1

